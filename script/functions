#!/bin/bash

if [ "$GITHUB_REF" == "refs/heads/develop" ]; then
    CD_BASEURL="https://clouddocs.f5networks.net/training/community"
    S3_BUCKET="clouddocs.f5networks.net"
elif [ "$GITHUB_REF" == "refs/heads/master" ]; then
    CD_BASEURL="https://clouddocs.f5.com/training/community"
    S3_BUCKET="clouddocs.f5.com"
fi

# Run a command.  Print error and exit if exitcode > 0
function run {
    "$@"
    local status=$?
    if [ $status -ne 0 ]; then
        echo "error with $1" >&2
        exit $status
    fi
    return $status
}

# Build lab submodules.  Set MAKE_OPT to pass addtional options/targets to make
function buildlabs {

    if [[ ! -z "$SKIP_BUILDLABS" ]]; then
        return
    fi

    if [[ -z "$BUILD_LIST" ]]; then
        BUILD_LIST=`ls labs`
    fi

    STATS_OPTIONS="-i"

    if [[ ! -z "$BUILD_ALL" ]]; then
        STATS_OPTIONS=""
    fi

    echo "BUILD_LIST=$BUILD_LIST"

    run mkdir -p hash

    for l in ${BUILD_LIST}
    do
        REPOROOT=$PWD
        LABREPOROOT="$PWD/labs/$l"
        DOCROOT="$LABREPOROOT/docs"

        RESPONSE=`curl -u clouddocs_staging:$CLOUDDOCS_PASSWD --write-out %{http_code} -s -o $REPOROOT/hash/$l "$CD_BASEURL/$l/buildhash.sha1"`
        CUR_HASH=`cd $LABREPOROOT && git rev-parse HEAD`
        LIVE_HASH=`cat $REPOROOT/hash/$l`
        if [[ "$RESPONSE" == "200" && "$CUR_HASH" == "$LIVE_HASH" ]]; then
            if [[ -z "$BUILD_ALL" ]]; then
                continue
            fi
        fi

        run mkdir -p "_build/community/$l/html"
        run script/containthedocs sudo pip install -r $LABREPOROOT/requirements.txt -U ; make -C "$DOCROOT" clean html
        run cd $LABREPOROOT
        run git rev-parse HEAD >> "$REPOROOT/_build/community/$l/buildhash.sha1"
        run cd $REPOROOT
        run cp -Rf "$DOCROOT"/_build/html/* "_build/community/$l/html"
        run cd "_build/community/$l"
        run ln -s html $l
        run zip -rq9 $l.zip $l/*
        run rm $l
        run cd $REPOROOT

        run aws s3 sync --delete _build/community/$l s3://$S3_BUCKET/training/community/$l
    done
}

# Build the root lab index
function buildroot {
    run mkdir -p _build/community/
    run cp -Rf landing/* _build/community/
    run script/containthedocs script/genindex.py landing/js/class_data.json landing/index.html _build/community/index.html

    run aws s3 sync --delete _build/community/css s3://$S3_BUCKET/training/community/css
    run aws s3 sync --delete _build/community/js s3://$S3_BUCKET/training/community/js
    run aws s3 cp _build/community/index.html s3://$S3_BUCKET/training/community/index.html
    run aws s3 cp _build/community/github_search.html s3://$S3_BUCKET/training/community/github_search.html

    if [ "$GITHUB_REF" == "refs/heads/master" ]; then
        run aws cloudfront create-invalidation --distribution-id $AWS_DIST_ID --paths /training/community
    fi
}

# Clean up the _build directory
function cleanbuild {
    run rm -Rf $PWD/_build/
    run mkdir -p _build
}
